{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vilalin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vilalin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 89527\n",
      "vocab length after removing stopwords: 89356\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "with open('./aclImdb/imdb.vocab', 'r') as f:\n",
    "    vocab = f.read()\n",
    "\n",
    "vocab = vocab.split('\\n')\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "vocab = list(set(vocab).difference(set(stopwords.words('english'))))\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "vocab_dic = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./aclImdb/train/pos/4715_9.txt', 1), ('./aclImdb/train/pos/12390_8.txt', 1)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "review_pos = glob.glob(\"./aclImdb/train/pos/*.txt\")\n",
    "review_neg = glob.glob(\"./aclImdb/test/neg/*.txt\")\n",
    "review_all = review_pos + review_neg\n",
    "y = [1]*len(review_pos) + [0]*len(review_neg)\n",
    "\n",
    "review_pairs = list(zip(review_all, y))\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    \n",
    "    with open(review_path, 'r') as f:\n",
    "        review = f.read()\n",
    "        \n",
    "    review = re.sub('[^a-zA-Z]',' ',review)\n",
    "    review = nltk.word_tokenize(review)\n",
    "    review = list(set(review).difference(set(stopwords.words('english'))))\n",
    "    \n",
    "    return review\n",
    "    \n",
    "\n",
    "def generate_vec(review, vocab_dic):\n",
    "    doc_vec = []\n",
    "    for word in review:\n",
    "        if vocab_dic.get(word):\n",
    "            doc_vec.append(vocab_dic.get(word))\n",
    "            \n",
    "    return torch.tensor(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    def __init__(self, data_dirs, vocab):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data_dirs[idx]\n",
    "        review = pair[0]\n",
    "        review = load_review(review)\n",
    "        review = generate_vec(review, self.vocab)\n",
    "        \n",
    "        return review, pair[1]\n",
    "    \n",
    "\n",
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "\n",
    "    corpus, labels = zip(*batch) \n",
    "    \n",
    "    ### create pads for corpus ###\n",
    "    lengths = [len(x) for x in corpus]\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    batch_corpus = []\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        # pad corpus\n",
    "        tmp_pads = torch.zeros(max_length)\n",
    "        tmp_pads[:lengths[i]] = corpus[i]\n",
    "        tmp_pads.view(-1, 1)\n",
    "        batch_corpus.append(tmp_pads.view(1,-1))\n",
    "\n",
    "    return torch.cat(batch_corpus,dim=0), torch.tensor(labels) , torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3538., 31017., 69236., 78885., 84516., 85591.,  5148., 34898.,  2542.,\n",
       "          87308., 27326., 45838., 52731., 63309., 43223., 71757., 43719., 87126.,\n",
       "          70920., 38377., 49836., 66873., 60226.,  1274.,  4433., 17451., 56765.,\n",
       "          28191., 49152., 10695., 77309., 69413., 84732., 85616., 79260., 84735.,\n",
       "          55510., 50556., 82274., 50372., 76820., 19909., 58685., 70265., 26821.,\n",
       "          30185., 32441., 23291.,  2910., 58695., 18858., 17985., 72478., 74933.,\n",
       "          31066., 56457., 48304., 24152., 36868., 21378.,  9353., 73354., 17144.,\n",
       "          31074., 57972., 82821., 77696., 74414., 87837., 50756., 88348., 22800.,\n",
       "          81378., 30404., 59550., 61014., 80023., 79505., 41674., 15203., 40257.,\n",
       "          71973., 12032., 62111., 26204., 53531., 16083., 19255., 59929., 58914.,\n",
       "          60843., 19448.,  8514., 16462., 46298., 53543., 77033., 66575.,  5057.,\n",
       "           8691., 66095., 75158., 18378., 75160., 49912., 27248., 37073., 76535.,\n",
       "          38306., 72528., 87417., 65731., 67830., 50967., 43322., 67511., 17201.,\n",
       "          50102., 54233.,  7360., 40808., 74306., 28834., 16495., 34654., 48894.,\n",
       "          71700., 62324., 88230., 60891., 31616., 80260., 11361., 20920., 86574.,\n",
       "          51875., 29905., 84851., 88623., 33406., 21635., 67170.,  9434., 40323.,\n",
       "          62524., 53054., 63642., 25581., 87758.,  5117., 55835., 70063., 65429.,\n",
       "          86593., 62904., 10295., 57744.,  2515., 27304., 69862., 76264., 76940.,\n",
       "           6518.,  7568., 15992., 39316., 67379., 35566.],\n",
       "         [ 3538.,  6114., 55374., 13878., 82714., 16663., 23291., 42616., 68646.,\n",
       "          75996., 51169., 58695., 28484., 50218., 34308., 75516., 77033., 49647.,\n",
       "          66665., 47508.,  2156.,  8691., 21635., 66095., 40324., 34513., 13309.,\n",
       "           7261., 83810.,  5117., 80094., 33193.,  5592., 26485., 55449., 54344.,\n",
       "          27022., 40982., 15330., 43056., 72993., 59962., 69379., 65478., 65360.,\n",
       "          71293., 73254., 12032.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.],\n",
       "         [42301., 44056., 29396., 48417.,  3545., 46391., 49647., 52408., 85956.,\n",
       "          66665.,   567., 11629., 53109., 43405., 67220., 17451., 73862.,  6915.,\n",
       "           8277., 50372., 58128., 26658., 70265., 84560., 58695.,  5004., 32776.,\n",
       "           4461., 53339., 17144., 13589., 14115., 22961., 88348., 11854., 61935.,\n",
       "          68674., 70468., 88555.,  5038., 15203., 18530., 16083., 55374., 19448.,\n",
       "          72689., 57351., 22622., 15760., 75516., 82515., 56661., 68838., 37073.,\n",
       "          66274., 80056., 32315., 87417., 71347., 40982., 29876., 59100., 67514.,\n",
       "          32331., 63614., 55597., 40135., 27639., 55810., 39620., 31449., 24211.,\n",
       "          74320., 64895., 76392., 38488., 67164., 58966., 69684., 27665., 28306.,\n",
       "          84851., 76079., 23210., 69208., 21635., 63990., 62524., 34513., 40323.,\n",
       "           9434., 83810., 87758.,  5117., 41393., 24590., 43687., 50311., 86593.,\n",
       "           5126., 83823.,  7568., 35922., 37311., 14218., 61456., 43372., 24246.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.],\n",
       "         [64396., 75945., 72638., 32441., 23291., 86574.,  2261., 20873., 17144.,\n",
       "          43604., 40323., 66095., 16098., 81203., 37073., 11163., 74857.,  5126.,\n",
       "          42831., 36430., 73646., 75546., 76692., 77669., 28834.,  7915.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.]]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([168,  48, 108,  26]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "custom_dataset = dataset(review_pairs, vocab_dic)\n",
    "custom_dataloader = DataLoader(dataset=custom_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "next(iter(custom_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
