{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUAjcc2h3Ggm"
   },
   "source": [
    "### 作業目的: 熟練Pytorch Dataset與DataLoader進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR-2XDR53Ggn"
   },
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 7.0MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting joblib (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl (302kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 5.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/9e/b956e48125b2034705841f2040d07374748bec107c46de07555f6b97774a/regex-2020.11.13-cp36-cp36m-macosx_10_9_x86_64.whl (284kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 5.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/vilalin/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from nltk) (4.52.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/vilalin/Library/Caches/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, nltk\n",
      "Successfully installed click-7.1.2 joblib-1.0.0 nltk-3.5 regex-2020.11.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NSD0BKPD3Ggn",
    "outputId": "d66df6e1-7e9e-4576-e13c-2dff4bdd603d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vilalin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vilalin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_uUsDAR3Ggs"
   },
   "source": [
    "### 探索資料與資料前處理\n",
    "在train資料中，有分成pos(positive)與neg(negative)，分別為正評價與負評價，此評價即為label。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oeoqU1O63Ggs",
    "outputId": "8ffbacc8-65ee-4f5d-e4bc-a610322f699e"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'LazyCorpusLoader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0fd7b4a96df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"vocab length before removing stopwords: {len(vocab)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m###<your code>###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"vocab length after removing stopwords: {len(vocab)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'LazyCorpusLoader' has no len()"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "vocab = stopwords\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "###<your code>###\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "### <your code>###\n",
    "vocab_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GMtQY5Eq3Ggv",
    "outputId": "ee0315a5-277e-45f6-e164-21c5346d8ff3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-612d06ef0797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m###<your code>###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total reviews: {len(review_pairs)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'review_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "###<your code>###\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzt0HH7f3Ggy"
   },
   "source": [
    "### 建立Dataset與DataLoader讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量BoW的函式\n",
    "(generate_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K75Uoit_3Ggy"
   },
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    \n",
    "    ###<your code>###\n",
    "        \n",
    "    #移除non-alphabet符號、贅字與tokenize\n",
    "    ###<your code>###\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpLFc_kV3Gg1"
   },
   "outputs": [],
   "source": [
    "def generate_bow(review, vocab_dic):\n",
    "    bag_vector = np.zeros(len(vocab_dic))\n",
    "    for word in review:\n",
    "        if vocab_dic.get(word):\n",
    "            bag_vector[vocab_dic.get(word)] += 1\n",
    "            \n",
    "    return bag_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpRZOSzS3Gg3"
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    def __init__(self, data_dirs, vocab):\n",
    "        ###<your code>###\n",
    "\n",
    "    def __len__(self):\n",
    "        ###<your code>###\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ###<your code>###\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPFVLTzy3Gg6",
    "outputId": "bdf581ea-f6de-4534-b5d6-8a9d6cb92e26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 0., 0.]), 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立客製化dataset\n",
    "custom_dst = dataset()\n",
    "custom_dst[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuvhF5cZ3Gg9",
    "outputId": "12b11c35-0a11-40f3-d523-50120a3c638c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " tensor([0, 0, 1, 0])]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立dataloader\n",
    "###<your code>###\n",
    "next(iter(custom_dataloader))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "以Pytorch進行基礎資料讀取_作業.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
